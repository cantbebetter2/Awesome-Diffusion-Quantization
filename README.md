# Awesome-Diffusion-Quantization [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A list of papers, docs, codes about diffusion quantization. This repo collects various quantization methods for the Diffusion Models.  Welcome to PR the works (papers, repositories) missed by the repo. 

## Contents

* [Papers](#Papers)
  * [2025](#2025)
  * [2024](#2024)
  * [2023](#2023)

## Papers

### 2025

* [[ICLR]](https://arxiv.org/abs/2406.02540) ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation [[code]](https://github.com/thu-nics/ViDiT-Q)![](https://img.shields.io/github/stars/thu-nics/ViDiT-Q)
* [[ICLR]](https://arxiv.org/abs/2411.05007) SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models [[code]](https://github.com/mit-han-lab/nunchaku)![](https://img.shields.io/github/stars/mit-han-lab/nunchaku)
* [[ICLR]](https://arxiv.org/abs/2404.05662) BinaryDM: Accurate Weight Binarization for Efficient Diffusion Models [[code]](https://github.com/Xingyu-Zheng/BinaryDM)![](https://img.shields.io/github/stars/Xingyu-Zheng/BinaryDM)
* [[ICLR]](https://arxiv.org/abs/2410.02367) SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration [[code]](https://github.com/thu-ml/SageAttention)![](https://img.shields.io/github/stars/thu-ml/SageAttention)
* [[CVPR]](https://arxiv.org/abs/2406.17343) Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers [[code]](https://github.com/Juanerx/Q-DiT)![](https://img.shields.io/github/stars/Juanerx/Q-DiT)
* [[CVPR]](https://arxiv.org/abs/2503.01323) CacheQuant: Comprehensively Accelerated Diffusion Models [[code]](https://github.com/BienLuky/CacheQuant)![](https://img.shields.io/github/stars/BienLuky/CacheQuant)
* [[CVPR]](https://arxiv.org/abs/2411.17106) PassionSR: Post-Training Quantization with Adaptive Scale in One-Step Diffusion based Image Super-Resolution [[code]](https://github.com/libozhu03/PassionSR)![](https://img.shields.io/github/stars/libozhu03/PassionSR)
* [[ICML]](https://arxiv.org/abs/2505.22167) Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers [[code]](https://github.com/cantbebetter2/Q-VDiT)![](https://img.shields.io/github/stars/cantbebetter2/Q-VDiT)
* [[ICML]](https://arxiv.org/abs/2411.10958) SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization [[code]](https://github.com/thu-ml/SageAttention)![](https://img.shields.io/github/stars/thu-ml/SageAttention)
* [[WACV]](https://arxiv.org/abs/2409.07756) DiTAS: Quantizing Diffusion Transformers via Enhanced Activation Smoothing [[code]](https://github.com/DZY122/DiTAS)![](https://img.shields.io/github/stars/DZY122/DiTAS)
* [[ISCAS]](https://arxiv.org/abs/2504.07998) CDM-QTA: Quantized Training Acceleration for Efficient LoRA Fine-Tuning of Diffusion Model 
* [[Arxiv]](https://arxiv.org/abs/2505.11497) QVGen: Pushing the Limit of Quantized Video Generative Models
* [[Arxiv]](https://arxiv.org/abs/2503.06564) TR-DQ: Time-Rotation Diffusion Quantization
* [[Arxiv]](https://arxiv.org/abs/2503.06930) Post-Training Quantization for Diffusion Transformer via Hierarchical Timestep Grouping
* [[Arxiv]](https://arxiv.org/abs/2502.04056) TQ-DiT: Efficient Time-Aware Quantization for Diffusion Transformers
* [[Arxiv]](https://arxiv.org/pdf/2503.15465) FP4DiT: Towards Effective Floating Point Quantization for Diffusion Transformers [[code]](https://github.com/cccrrrccc/FP4DiT) ![](https://img.shields.io/github/stars/cccrrrccc/FP4DiT)
* [[Arxiv]](https://arxiv.org/abs/2503.06545) QuantCache: Adaptive Importance-Guided Quantization with Hierarchical Latent and Layer Caching for Video Generation [[code]](https://github.com/JunyiWuCode/QuantCache) ![](https://img.shields.io/github/stars/JunyiWuCode/QuantCache)
* [[Arxiv]](https://arxiv.org/abs/2505.02242) Quantizing Diffusion Models from a Sampling-Aware Perspective
* [[Arxiv]](https://arxiv.org/abs/2505.11497) QVGen: Pushing the Limit of Quantized Video Generative Models
* [[Arxiv]](https://arxiv.org/abs/2503.05584) QArtSR: Quantization via Reverse-Module and Timestep-Retraining in One-Step Diffusion based Image Super-Resolution [[code]](https://github.com/libozhu03/QArtSR) ![](https://img.shields.io/github/stars/libozhu03/QArtSR)
* [[Arxiv]](https://arxiv.org/abs/2503.02508) Q&C: When Quantization Meets Cache in Efficient Image Generation 
* [[Arxiv]](https://arxiv.org/abs/2505.21591) Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning
* [[Arxiv]](https://arxiv.org/abs/2505.18663) DVD-Quant: Data-free Video Diffusion Transformers Quantization [[code]](https://github.com/lhxcs/DVD-Quant) ![](https://img.shields.io/github/stars/lhxcs/DVD-Quant)

### 2024

* [[ICLR]](https://arxiv.org/abs/2310.03270) EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models [[code]](https://github.com/ThisisBillhe/EfficientDM)![](https://img.shields.io/github/stars/ThisisBillhe/EfficientDM)
* [[CVPR]](https://arxiv.org/abs/2311.16503) TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models [[code]](https://github.com/ModelTC/TFMQ-DM)![](https://img.shields.io/github/stars/ModelTC/TFMQ-DM)
* [[CVPR]](https://arxiv.org/abs/2305.18723) Towards Accurate Post-training Quantization for Diffusion Models [[code]](https://github.com/ChangyuanWang17/APQ-DM)![](https://img.shields.io/github/stars/ChangyuanWang17/APQ-DM)
* [[ECCV]](https://arxiv.org/pdf/2405.17873) MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization [[code]](https://github.com/thu-nics/MixDQ)![](https://img.shields.io/github/stars/thu-nics/MixDQ)
* [[ECCV]](https://arxiv.org/abs/2407.03917) Timestep-Aware Correction for Quantized Diffusion Models 
* [[ECCV]](https://arxiv.org/abs/2311.06322v3) Post-training Quantization for Text-to-Image Diffusion Models with Progressive Calibration and Activation Relaxing [[code]](https://github.com/tsa18/PCR)![](https://img.shields.io/github/stars/tsa18/PCR)
* [[ECCV]](https://arxiv.org/abs/2401.04339) Memory-Efficient Fine-Tuning for Quantized Diffusion Model [[code]](https://github.com/ugonfor/TuneQDM)![](https://img.shields.io/github/stars/ugonfor/TuneQDM)
* [[NeurIPS]](https://arxiv.org/pdf/2405.16005) PTQ4DiT: Post-training Quantization for Diffusion Transformers [[code]](https://github.com/adreamwu/PTQ4DiT)![](https://img.shields.io/github/stars/adreamwu/PTQ4DiT)
* [[NeurIPS]](https://arxiv.org/abs/2406.04333) BitsFusion: 1.99 bits Weight Quantization of Diffusion Model [[code]](https://github.com/snap-research/BitsFusion)![](https://img.shields.io/github/stars/snap-research/BitsFusion)
* [[NeurIPS]](https://arxiv.org/abs/2405.14854) TerDiT: Ternary Diffusion Models with Transformers [[code]](https://github.com/Lucky-Lance/TerDiT)![](https://img.shields.io/github/stars/Lucky-Lance/TerDiT)
* [[NeurIPS]](https://arxiv.org/abs/2406.05723) Binarized Diffusion Model for Image Super-Resolution [[code]](https://github.com/zhengchen1999/BI-DiffSR)![](https://img.shields.io/github/stars/zhengchen1999/BI-DiffSR)
* [[NeurIPS]](https://arxiv.org/abs/2412.05926) BiDM: Pushing the Limit of Quantization for Diffusion Models [[code]](https://github.com/Xingyu-Zheng/BiDM)![](https://img.shields.io/github/stars/Xingyu-Zheng/BiDM)
* [[NeurIPS]](https://proceedings.neurips.cc/paper_files/paper/2024/hash/615675cc6e94ddb1a783904fb178b5f6-Abstract-Conference.html) StepbaQ: Stepping backward as Correction for Quantized Diffusion Models 
* [[AAAI]](https://arxiv.org/abs/2412.11549) MPQ-DM: Mixed Precision Quantization for Extremely Low Bit Diffusion Models [[code]](https://github.com/cantbebetter2/MPQ-DM)![](https://img.shields.io/github/stars/cantbebetter2/MPQ-DM)
* [[AAAI]](https://arxiv.org/abs/2412.14628) Qua2SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models [[code]](https://github.com/Ascend-Research/Qua2SeDiMo)![](https://img.shields.io/github/stars/Ascend-Research/Qua2SeDiMo)
* [[AAAI]](https://arxiv.org/abs/2412.16700) TCAQ-DM: Timestep-Channel Adaptive Quantization for Diffusion Models 
* [[AAAI]](https://ojs.aaai.org/index.php/AAAI/article/view/34039) Optimizing Quantized Diffusion Models via Distillation with Cross-Timestep Error Correction 
* [[Arxiv]](https://arxiv.org/abs/2402.03666) QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning [[code]](https://github.com/hatchetProject/QuEST)![](https://img.shields.io/github/stars/hatchetProject/QuEST)
* [[Arxiv]](https://arxiv.org/abs/2405.19751) HQ-DiT: Efficient Diffusion Transformer with FP4 Hybrid Quantization 
* [[Arxiv]](https://arxiv.org/abs/2408.17131) VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers
* [[Arxiv]](https://arxiv.org/abs/2411.14172) TaQ-DiT: Time-aware Quantization for Diffusion Transformers [[code]](https://github.com/yhwangs/TQ-DiT)![](https://img.shields.io/github/stars/yhwangs/TQ-DiT)

### 2023

* [[ICCV]](https://arxiv.org/abs/2302.04304) Q-Diffusion: Quantizing Diffusion Models [[code]](https://github.com/Xiuyu-Li/q-diffusion)![](https://img.shields.io/github/stars/Xiuyu-Li/q-diffusion)
* [[CVPR]](https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Post-Training_Quantization_on_Diffusion_Models_CVPR_2023_paper.pdf) Post-training Quantization on Diffusion Models [[code]](https://github.com/42Shawn/PTQ4DM)![](https://img.shields.io/github/stars/42Shawn/PTQ4DM)
* [[NeurIPS]](https://arxiv.org/pdf/2305.10657) PTQD: Accurate Post-Training Quantization for Diffusion Models [[code]](https://github.com/ziplab/PTQD)![](https://img.shields.io/github/stars/ziplab/PTQD)
* [[NeurIPS] ](https://proceedings.neurips.cc/paper_files/paper/2023/hash/f1ee1cca0721de55bb35cf28ab95e1b4-Abstract-Conference.html)Q-DM: An Efficient Low-bit Quantized Diffusion Model 
* [[NeurIPS]](https://arxiv.org/abs/2306.02316) Temporal Dynamic Quantization for Diffusion Models 

